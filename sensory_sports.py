# -*- coding: utf-8 -*-
"""Sensory_Sports.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1duzQjTZXqh3cx38HE4rzbYJfhlWwIzyY

# EEG Analysis Model
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""**Load Data from CSV**"""

file_path = '/content/emotions.csv'
data = pd.read_csv(file_path)

data.head()

"""**Data Exploration and Analysis**"""

print("Data Shape:", data.shape)
print("Column Names:", data.columns)

label_counts = data['label'].value_counts()
print("Emotional Labels Distribution:", label_counts)

plt.figure(figsize=(8, 6))
label_counts.plot(kind='bar', color=['red', 'gray', 'green'])
plt.title('Distribution of Emotional Labels')
plt.xlabel('Emotion Label')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

"""**Data Preparation**"""

X = data.drop(columns=['label'])
y = data['label']

y = y.map({'NEGATIVE': 0, 'NEUTRAL': 1, 'POSITIVE': 2})

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**Build Deep Learning Model (LSTM)**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

"""**Build LSTM Model**"""

model = Sequential()

model.add(LSTM(units=50, return_sequences=False, input_shape=(X_train.shape[1], 1)))

model.add(Dropout(0.2))

model.add(Dense(units=3, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

"""**Train LSTM Model**"""

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

"""**Evaluate Model**"""

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Accuracy: {accuracy * 100:.2f}%')

plt.plot(history.history['accuracy'], label='Accuracy (Training)')
plt.plot(history.history['val_accuracy'], label='Accuracy (Validation)')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""# Pitch Analysis

**Clone YOLOv7 Repository**
"""

!git clone https://github.com/WongKinYiu/yolov7.git

"""**Install Required Dependencies**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd yolov7
!pip install -r requirements.txt

"""**Download Pre-trained YOLOv7 Weights**"""

!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/yolov7
!sed -i 's/ckpt = torch.load(w, map_location=map_location)/ckpt = torch.load(w, map_location=map_location, weights_only=False)/' models/experimental.py

"""**Run YOLOv7 Detection on a Match Video**"""

!python detect.py --weights yolov7.pt --conf 0.25 --source "/content/test_video.mp4" --save-txt --save-conf --project results --name pitch_analysis_detection

"""**Display Normal Video**"""

import cv2
from google.colab.patches import cv2_imshow

normal_video_path = '/content/test_video.mp4'
cap = cv2.VideoCapture(normal_video_path)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    cv2_imshow(frame)
    if cv2.waitKey(25) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()

"""**Display Analyzed  Video**"""

import cv2
from google.colab.patches import cv2_imshow

analyzed_video_path = '/content/yolov7/results/pitch_analysis_detection3/test_video.mp4'
cap = cv2.VideoCapture(analyzed_video_path)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    cv2_imshow(frame)
    if cv2.waitKey(25) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()

"""# Multi-Sensory Integration Model

**Install Dependencies**
"""

!pip install pygame
!pip install simpleaudio

"""**Import Libraries and Define EEG Simulation Function**"""

import pygame
import sys
import time
import random
import simpleaudio as sa

def get_eeg_signal_simulation():
    """
    Simulate an EEG signal reading.
    This function returns a random float between 0 and 1 representing the user's neural activity.
    """
    return random.uniform(0, 1)

"""**Define Deaf Mode (Visual Display with Pygame)**"""

def run_deaf_mode():
    """
    Deaf Mode: Display a continuous arrow on the screen.
    The arrow moves up and down continuously over a gradient background.
    """
    pygame.init()
    # Set screen dimensions
    screen_width, screen_height = 800, 600
    screen = pygame.display.set_mode((screen_width, screen_height))
    pygame.display.set_caption("Deaf Mode - Continuous Arrow")

    # Define background gradient colors
    background_color_top = (5, 10, 40)
    background_color_bottom = (20, 50, 100)

    # Initial arrow parameters
    arrow_color = (255, 0, 0)
    arrow_x = screen_width // 2
    arrow_y = screen_height // 2
    arrow_direction = 1  # 1 means moving down initially; -1 will reverse direction

    clock = pygame.time.Clock()

    running = True
    while running:
        # Handle pygame events
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False

        # Simulate EEG reading (this can later be replaced with real EEG values)
        eeg_value = get_eeg_signal_simulation()

        # Move the arrow up or down. Modify the movement based on EEG value if desired.
        arrow_y += arrow_direction * 5
        if arrow_y < 50 or arrow_y > screen_height - 50:
            arrow_direction *= -1  # Reverse direction when reaching boundaries

        # Draw the gradient background
        for y in range(screen_height):
            r = background_color_top[0] + (background_color_bottom[0] - background_color_top[0]) * y / screen_height
            g = background_color_top[1] + (background_color_bottom[1] - background_color_top[1]) * y / screen_height
            b = background_color_top[2] + (background_color_bottom[2] - background_color_top[2]) * y / screen_height
            pygame.draw.line(screen, (int(r), int(g), int(b)), (0, y), (screen_width, y))

        # Draw the arrow as a simple triangle
        arrow_size = 30
        arrow_points = [
            (arrow_x, arrow_y - arrow_size),
            (arrow_x - arrow_size // 2, arrow_y + arrow_size // 2),
            (arrow_x + arrow_size // 2, arrow_y + arrow_size // 2)
        ]
        pygame.draw.polygon(screen, arrow_color, arrow_points)

        pygame.display.flip()
        clock.tick(30)  # Limit to 30 FPS

    pygame.quit()

"""**Define Blind Mode (Spatial Audio Playback)**"""

def run_blind_mode():
    """
    Blind Mode: Play a spatial audio file.
    This is a simplified simulation. The audio file is expected to be preprocessed
    for spatial effects (e.g., panned to left/right) and uploaded to Colab.
    """
    print("Blind Mode activated - Playing spatial audio.")
    audio_file_path = "/content/spatial_audio.wav"  # Replace with your audio file path
    try:
        wave_obj = sa.WaveObject.from_wave_file(audio_file_path)
        play_obj = wave_obj.play()
        play_obj.wait_done()  # Wait until audio playback is finished
    except Exception as e:
        print("Error playing audio:", e)

"""**Main Execution – Mode Selection**"""

def main():
    """
    Main function to select mode.
    Enter '1' for Deaf Mode and '2' for Blind Mode.
    """
    print("Select Mode:")
    print("1 - Deaf Mode (Visual Arrow)")
    print("2 - Blind Mode (Spatial Audio)")

    choice = input("Enter your choice (1 or 2): ").strip()

    if choice == "1":
        run_deaf_mode()
    elif choice == "2":
        run_blind_mode()
    else:
        print("Invalid choice. Please choose 1 or 2.")

if __name__ == "__main__":
    main()

"""# Capture Pygame frames to images, then display them in Colab

**Install Dependencies**
"""

!pip install pygame simpleaudio imageio

"""**Define Functions for Tone Generation and Deaf Mode Simulation (Stock Chart Mode)**"""

import pygame
import sys
import random
import simpleaudio as sa
import imageio
import numpy as np
import os
import time

def generate_tone(freq=440, duration=0.05, volume=0.3, sample_rate=44100):
    """
    Generate a sine wave tone with the given frequency, duration, and volume.
    """
    t = np.linspace(0, duration, int(sample_rate * duration), False)
    wave = np.sin(freq * t * 2 * np.pi)
    audio = (wave * (2**15 - 1) * volume).astype(np.int16)
    return audio

def run_deaf_mode_stock(num_frames=100):
    """
    Simulate a stock market indicator:
    - A random walk is generated between the 'StopLoss' (y=100) and 'Target' (y=500) lines.
    - The arrow is drawn at the current position along the chart with slight jitter for vibration.
    - A tone is played at each frame with frequency modulated by the arrow's vertical position.
    - Each frame is saved as an image in a folder "stock_frames".
    """
    pygame.init()
    width, height = 800, 600
    screen = pygame.display.set_mode((width, height))
    pygame.display.set_caption("Stock Market Indicator Simulation")
    clock = pygame.time.Clock()

    output_dir = "stock_frames"
    os.makedirs(output_dir, exist_ok=True)

    bg_color = (0, 0, 0)
    line_color = (255, 255, 255)  # white
    chart_color = (0, 255, 0)      # green for the chart line
    arrow_color = (255, 0, 0)      # red for the arrow

    stoploss_y = 100  # upper boundary
    target_y = 500    # lower boundary

    # Generate random walk data for the "stock price"
    data = []
    current_value = (stoploss_y + target_y) / 2.0
    for i in range(num_frames):
        step = random.uniform(-5, 5)
        current_value += step
        current_value = max(stoploss_y, min(current_value, target_y))
        data.append(current_value)

    x_step = width / (num_frames - 1)
    prev_value = data[0]

    # Jitter range to simulate vibration visually
    jitter_range = 3

    for frame_idx in range(num_frames):
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                return output_dir, num_frames

        screen.fill(bg_color)

        # Draw StopLoss (upper) and Target (lower) lines
        pygame.draw.line(screen, line_color, (0, stoploss_y), (width, stoploss_y), 2)
        pygame.draw.line(screen, line_color, (0, target_y), (width, target_y), 2)

        # Draw the chart line up to the current frame (simulate stock price movement)
        if frame_idx > 0:
            points = []
            for i in range(frame_idx):
                x = i * x_step
                y = data[i]
                points.append((x, y))
            if len(points) > 1:
                pygame.draw.lines(screen, chart_color, False, points, 2)

        # Current arrow position on the chart
        arrow_x = frame_idx * x_step
        arrow_y = data[frame_idx]

        # Determine arrow tilt based on change in stock price
        delta = arrow_y - prev_value
        prev_value = arrow_y

        # Add random jitter for vibration effect (visual)
        jitter_x = random.uniform(-jitter_range, jitter_range)
        jitter_y = random.uniform(-jitter_range, jitter_range)
        arrow_x_vib = arrow_x + jitter_x
        arrow_y_vib = arrow_y + jitter_y

        arrow_size = 15
        if delta >= 0:
            # Arrow pointing upward (if price rising)
            arrow_points = [
                (arrow_x_vib, arrow_y_vib),
                (arrow_x_vib - arrow_size, arrow_y_vib + arrow_size),
                (arrow_x_vib + arrow_size, arrow_y_vib + arrow_size)
            ]
        else:
            # Arrow pointing downward (if price falling)
            arrow_points = [
                (arrow_x_vib, arrow_y_vib),
                (arrow_x_vib - arrow_size, arrow_y_vib - arrow_size),
                (arrow_x_vib + arrow_size, arrow_y_vib - arrow_size)
            ]
        pygame.draw.polygon(screen, arrow_color, arrow_points)

        # Play a tone (vibration effect)
        # Frequency modulated based on arrow_y: higher freq when near top (stoploss), lower when near target.
        freq = 300 + (arrow_y - stoploss_y) * (200 / (target_y - stoploss_y))
        try:
            tone = generate_tone(freq=freq, duration=0.05, volume=0.3)
            play_obj = sa.play_buffer(tone, 1, 2, 44100)
            play_obj.wait_done()
        except Exception as e:
            print("Audio playback error:", e)

        pygame.display.flip()
        clock.tick(15)  # 15 FPS

        # Save the current frame as an image
        frame_data = pygame.surfarray.array3d(pygame.display.get_surface())
        frame_data = np.transpose(frame_data, (1, 0, 2))
        frame_path = os.path.join(output_dir, f"frame_{frame_idx:03d}.png")
        imageio.imsave(frame_path, frame_data)

    pygame.quit()
    return output_dir, num_frames

def main_deaf_mode():
    folder, total_frames = run_deaf_mode_stock(num_frames=100)
    print(f"Captured {total_frames} frames in folder: {folder}")

if __name__ == "__main__":
    main_deaf_mode()

"""**Convert Frames to GIF and MP4**"""

import imageio
import glob

def make_stock_gif():
    frames = sorted(glob.glob("stock_frames/frame_*.png"))
    images = []
    for f in frames:
        images.append(imageio.imread(f))
    imageio.mimsave("advanced_arrow.gif", images, fps=15)
    print("GIF saved as advanced_arrow.gif")

def make_stock_mp4():
    frames = sorted(glob.glob("stock_frames/frame_*.png"))
    writer = imageio.get_writer("advanced_arrow.mp4", fps=15)
    for f in frames:
        img = imageio.imread(f)
        writer.append_data(img)
    writer.close()
    print("MP4 saved as advanced_arrow.mp4")

make_stock_gif()
make_stock_mp4()

"""**Display the Resulting Animation**"""

from IPython.display import Image, Video, display

display(Image(filename="advanced_arrow.gif"))
display(Video("advanced_arrow.mp4", embed=True))

"""**Output Video With Sound**"""

from IPython.display import Video, display, HTML
display(HTML("<h1 style='color:white ; text-align:center;'>ملحوظة: الفيديو المخرج من الكود السابق بدون صوت بسبب طبيعة بيئة كولاب ، ولكن تم إنتاجه محلياً وتصويره</h1>"))
display(Video("/content/deef_effect_output.mp4", embed=True, width=320, height=240))

"""# Spatial Audio Converter

**Install Dependencies for 3D Audio Spatializer**
"""

!pip install pydub ffmpeg-python

"""**Process MP3 to 3D Spatial Audio**"""

from pydub import AudioSegment
import os
import math

input_audio_path = "/content/2D_audio_test.mp3"
audio = AudioSegment.from_file(input_audio_path, format="mp3")

spatialized_audio = AudioSegment.empty()
duration = len(audio)  # duration in milliseconds
segment_duration = 1000  # process in 1-second segments

for i in range(0, duration, segment_duration):
    segment = audio[i:i+segment_duration]
    pan_value = 0.8 * math.sin(2 * math.pi * i / duration)  # oscillates between -0.8 and +0.8
    segment = segment.pan(pan_value)
    spatialized_audio += segment

# Trim the audio to the first 22 seconds (22000 ms)
trimmed_audio = spatialized_audio[:22000]

output_audio_path = "/content/2D_audio_spatialized_trimmed.wav"
trimmed_audio.export(output_audio_path, format="wav")
print("Trimmed spatial audio file saved to:", output_audio_path)

"""**Display Spatial Audio Output**"""

from IPython.display import Audio, display
print("ملحوظة: أول 22 ثانية فقط تم عرضهم من الفيديو، نظرا لضيق الوقت")
display(Audio("/content/2D_audio_spatialized_trimmed.wav", autoplay=False))

"""**Display whole converted one**"""

# To display the whole audio
#from IPython.display import Audio, display

# display(Audio("/content/2D_audio_spatialized.wav", autoplay=False))

"""**More cleared 3D Audio of PlayGround**"""

print("مثال اخر لتوضيح الصوت ثلاثي الأبعاد بشكل أوضح")
display(Audio("/content/3D_playGround_audio.mp3", autoplay=False))

